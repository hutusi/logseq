-
- BERT = Bidirectional Encoder Representations from Transformers
-
- # 开始使用 BERT
- ## 1 Transformer概览
- ### 1.1 Transformer 简介
- RNN和LSTM都用于时序任务，如文本预测、机器翻译、文章生成等，问题是如何记录长期依赖（？）。
- Transformer为解决这个问题而生。
- Transformer依赖于注意力极致，摒弃了循环。
-
- Transformer基本工作原理：
- (文本) --> 编码器 --> (特征) --> 解码器 --> (文本)
-
-
- ### 1.2 理解编码器
- Transformer中编码器不止一个，而是由一组 N 个编码器串联而成。一个编码器的输出作为下一个编码器的输入，编码器的主要功能就是提取原句中的特征。
- 编码器可以分解为两个部分：
- 多头注意力层 --> 前馈网络层
-
- #### 1.2.1 自注意力机制
- 比如例句： A dog ate the food because it was hungry. 我们理解it代指dog，而非其他，那模型是如何判断出来的呢？这就要靠自注意力机制。
- 模型会计算每个单词的特征值：当计算每个词的特征值时，模型需要遍历每个词与句子中其他词的关系。在这个例子中，it 与 dog的关系最紧密。
-
- 嵌入向量：
- 假设输入原句为 I am good, 将每个词转化为嵌入向量，分别用 $$x_1$$, $$x_2$$, $$x_3$$ 代表三个词的嵌入向量：
- $$x_1 = [1.76, 2.22, ..., 6.66]$$
- $$x_2 = [7.77, 0.631, ..., 5.35] $$
- $$x_3 = [11.44, 10.10, ..., 3.33] $$
-
- 这样，该输入句就可以用一个矩阵（嵌入矩阵）来表示：
- ![image.png](../assets/image_1724508963883_0.png)
-
-
- 输入矩阵的维度为 [句子的长度 * 词嵌入向量维度]，如果词嵌入向量维度为512，则该输入矩阵的维度是[3*512]
- 而词嵌入向量维度是在模型设计是指定的，一般会根据计算资源、数据集大小、任务复杂性等因素来考虑。
-
- 根据输入矩阵X，再创建三个矩阵：查询矩阵Q、键矩阵K、值矩阵V。
- 为了创建这三个矩阵，我们需要先创建三个权重矩阵，分别为 $$W^Q$$、$$W^K$$、$$W^V$$
- 将输入矩阵X，分别乘以这三个权重矩阵，可得到查询矩阵、键矩阵、值矩阵，如下图示：
- ![image.png](../assets/image_1724509450575_0.png)
-
- 理解自注意力机制，自注意力机制包含四个步骤：
- 1、计算查询矩阵Q和键矩阵K的点积
- 因为计算矩阵点积，所以要将K转置，变成 $$K^T$$
- ![image.png](../assets/image_1724552500196_0.png)
-
-
-
- ![image.png](../assets/image_1724552584432_0.png)
- 通过查询向量和键向量的点积，可以了解单词之间的相似度。
-
- 2、将$$Q · K^T$$矩阵除以键向量维度的平方根，这样做的目的是获得稳定的梯度。
- ![image.png](../assets/image_1724553231039_0.png)
-
- 3、使用softmax函数对其结果进行归一化处理：
- softmax函数使得数组分布在0~1的范围内，且每一行的所有数之和等于1.
- ![image.png](../assets/image_1724553760888_0.png)
-
- 4、
-
-
-
- 本书作为《深度学习入门：基于Python的理论与实现》的续作，主要介绍了深度学习在自然语言处理（NLP）上的主要方法和模型。近来也在转型学习神经网络和深度学习，对于书中的一些公式还不太能理解，只能浅要总结如下：
- 1、神经网络有输入层、隐藏层和输出层。神经网络的推理可以理解为数据的正向传播，神经网络的学习可以理解为梯度数据的反向传播。使用计算图可以帮助理解各节点的正向传播和反向传播的计算过程。
- 2、自然语言处理（NLP）是深度学习的一个领域，目的是让计算机能理解人类语言。在NLP中，单词用向量表示，单词向量化的方法主要有两种：一种是基于计数的方法，一种是基于推理的方法。
- 3、基于计数的方法是对语料库中每个单词上下文单词出现的频数进行计数生成共现矩阵，然后转换为PPMI矩阵，再做降维处理，可以将巨大的稀疏向量转换为小的密集向量。基于计数的方法的问题是共现矩阵巨大，做降维处理的复杂度很高。
- 4、基于推理的方法一次只选取一小部分数据，即在mini-batch语料上学习，它基于上下文的单词推测中心词，就像完形填空。从模型视角看，就是输入上下文，模型输出各单词的出现概率。
- 5、RNN循环神经网络为了表达自然语言的序列，利用RNN创建的语言模型具有记忆能力。RNN具有环路，因此可以在内部记忆隐藏状态。RNN可以理解为多个RNN层连接起来的神经网络。
- 6、RNN学习过程中可能出现梯度爆炸（梯度变大超过一定阈值）或梯度消失（梯度变小接近0）的问题，影响模型学习。梯度裁剪（超过阈值后裁剪）对解决梯度爆炸有效，而LSTM、GRU等模型（Gated RNN）引入“门”的结构，解决梯度消失问题。LSTM即Long Short-Term Memory，意思是可以长时间维持短期记忆。LSTM有三个门：输入门、遗忘门、输出吗。门有专门的权重。
- 7、文本数据、音频数据、视频数据都是时序数据。基于RNN语言模型可以生成新的文本，通过组合两个RNN，可以将将一个时序数据转换为另一个时序数据（seq2seq），如机器翻译、语音识别等。seq2seq模型包含两个RNN模块：Encoder（编码器）和Decoder（解码器）。文本识别的seq2seq实现：Encoder由Embedding层和LSTM层组成，Embedding层将字符转换为字符向量，再输入LSTM层。Decoder由Embedding层、LSTM层和Affine层构成。如果将编码器从LSTM换成CNN（卷积神经网络），seq2seq就可以处理图像。
- 8、seq2seq的问题是编码器的输出是固定长度的向量，也就会导致在长文本处理时可能会发生有用的信息从向量中溢出。Attention机制是为了进一步强化seq2seq，基于Attention机制，使得seq2seq能像人一样，将“注意力”关注在重要信息上。Attention机制通过改造编码器和解码器来解决seq2seq的固定长度向量问题。
- 9、Transformer模型不用RNN，基于Attention构成，使用Self-Attention技巧。
-
-
-
- # 神经网络的复习
	- ## 数学和Python
		- 向量和矩阵、张量
		- 矩阵运算
		- 广播
	- ## 神经网络的推理
		- 神经网络的例子
			- ![image.png](../assets/image_1700384196845_0.png){:height 373, :width 629}
			- 每个神经元（图中O型节点）的输入是与它连接的上一层神经元的输出值与权重（与连接的两个神经元有关）相乘，并累加，再加上偏置（与上一层神经元无关）；这个值，再经过激活函数转换，成为该神经元的输出值。如下公式(x代表输入层的数据，w代表权重，b代表偏置)：
				- $$ h_1 = x_1w_{11} + x_2w_{21} + b_1 $$
				- 使用 sigmoid激活函数进行变换：
				- $$ a = sigmoid(h) $$
			- 联想：神经网络的推理所进行的处理相当于神经网络的正向传播；神经网络的学习则是反向传播。
				- 将神经网络的层实现为Python 类：
					- 所有的层都有 forward() 方法和 backward() 方法
					- 所有的层都有 params 和 grads 实例变量
				- 神经网络层结构：
				- ![Screenshot 2023-11-20 at 23.14.12.png](../assets/Screenshot_2023-11-20_at_23.14.12_1700493312652_0.png)
	- ## 神经网络的学习
		- 损失函数
			- 在神经网络的学习中，为了知道学习的如何，需要一个指标，这个指标就是损失(loss)。
			- 计算神经网络的损失要用到损失函数(loss function)，进行多类别分类的神经网络通常使用交叉熵误差(cross entropy error)作为损失函数。
			- ![image.png](../assets/image_1700493539753_0.png)
			- 可以将 Softmax 函数和 交叉熵误差的层实现为 Softmax with Loss 层：
			- ![image.png](../assets/image_1700493619661_0.png)
		- 导数和梯度：
			- 导数相当于函数的斜率
			- 将向量的各个元素的导数罗列在一起，就得到了梯度
		- 链式法则：
			- 理解误差反向传播法的关键是链式法则，链式法则是复合函数的求导法则，即复合函数的导数之积。
		- 计算图
			- 使用计算图，可以直观的把握计算过程。
		- 使用计算图对各种节点的正向传播和反向传播进行计算：
			- 加法节点
				- ![image.png](../assets/image_1700576144297_0.png)
			- 乘法节点
				- ![image.png](../assets/image_1700576185370_0.png)
			- 分支节点
				- ![image.png](../assets/image_1700576261718_0.png)
			- Repeat 节点
				- ![image.png](../assets/image_1700576298045_0.png)
			- Sum节点
				- ![image.png](../assets/image_1700576328524_0.png)
			- MatMul节点（Matrix Multiply）
				- ![image.png](../assets/image_1700576384548_0.png)
				- ![image.png](../assets/image_1700576408292_0.png)
		- 神经网络的学习步骤:
			- 步骤1：mini-batch
			- 步骤2：计算梯度
			- 步骤3：更新参数
			- 步骤4：重复1-3
		- 权重更新方法有很多，常见的有 随机梯度下降算法（SGD, Stochastic Gradient Descent）
	- ## 使用神经网络解决问题
	- ## 计算的高速化
		- 位精度：32位浮点数比64位浮点数计算快，16位浮点数比32位浮点数计算快
		- GPU并行计算
-
- # 自然语言和单词的分布式表示
	- ## 自然语言处理
		- NLP = Natural Language Processing
	- ## 同义词词典
		- 同义词词典可以用来在信息检索场景，如 WordNet
		- 同义词典的问题:
			- 难以顺应时代变化
			- 人力成本高
			- 无法表达单词的微妙差异
	- ## 基于计数的方法
		- 语料库（corpus）：大量的文本数据
		- 如何将单词表示为向量?
			- 思考：如何表示颜色？比如可以通过名称来表示，比如钴蓝等，这样有多少种颜色，就需要多少种名字。还有一种更好的方式方法，使用RGB三原色表示，这样可以通过三维向量来表示颜色。
			- 在自然语言处理领域，将关注能准确把握单词含义的向量表示，称为**分布式表示**。
		- **分布式假设**：单词本身没有含义，单词含义由它所在的上下文（语境）形成。上下文的大小（即周围单词有多少个）称为窗口大小。如下面例子中，关于goodbye时，窗口大小为2的上下文:
			- ![image.png](../assets/image_1700665357420_0.png)
		- 共现矩阵:
			- 例，对 'You say goodbye and I say hello.' 来进行向量表示，首先对它进行编ID:
			- {0: 'you', 1: 'say', 2: 'goodbye', 3: 'and', 4: 'i', 5: 'hello', 6: '.'}
			- 预设窗口大小为1，统计关注单词上下文出现的频数，如对say统计如下：
			- ![image.png](../assets/image_1700666791618_0.png)
			- 单词say可以表示为向量 [1, 0, 1, 0, 1, 1, 0]，对所有单词进行处理：
			- ![image.png](../assets/image_1700666870171_0.png)
			- 这种汇总所有单词的共现单词的形式，称为 共现矩阵 (co-occurence matrix)。有了这个矩阵，就可以获得各个单词的向量。
		- 向量间的相似度
			- 测量单词的向量表示的相似度方面，常用的方法有余弦相似度（cosine similarity）
	- ## 基于计数方法的改进
		- 点互信息
			- 共现矩阵存在的问题：比如某个语料中有很多 "...the car..."这样的短语，那么它们共现的次数将很大，如果只看单词共现次数，那the和car的相关性比 car和drive的相关性更强，显然是有问题的。
			- 为了解决这一问题，引入点互信息（Pointwise Mutual Information, PMI）这一指标：
				- $$PMI(x, y) = log_2\dfrac{P(x, y)}{P(x)P(y)}$$
			- P(x)表示x出现的概率，P(y)表示y出现的概率，P(x, y)表示x和y同时出现的概率。PMI值越高，表明相关性越强。上述例子中，由于 the 本身会出现很多，因此PMI值会被拉低。
			- PMI的问题：但当两词共现次数为0时， $$log_2=-infty$$，因此实际上会用正点互信息（Positive PMI, PPMI）:
				- $$PPMI(x, y) = max(0, PMI(x, y)$$
		- 降维
			- 降维（dimensionality reduction），就是减少向量维度。
			- 为什么要什么降维?
				- PPMI矩阵的问题是，随着语料库单词量增加，各单词向量的维度也会增加。但矩阵中很多元素都是0，因此说明向量中的绝大多数元素并不重要。因此需要使用一些降维方法来减少这种噪声：
					- 奇异值分解（Singular Value Decomposition, SVD）
- # word2vec
	- ## 基于推理的方法和神经网络
		- 用向量表示单词的方法大致有两种：基于计数的方法和基于推理的方法
		- 基于计数的方法的问题是语料库数据量大，生成共现矩阵过于庞大，执行SVD复杂度成比例增长。因此，替代的方法是基于推理的方法：该方法通常在mini-batch数据上学习，一次只需看一部分学习数据：
			- ![image.png](../assets/image_1700970905441_0.png)
		- 基于推理的方法的主要操作是“推理”，根据上下文来预测 ？ 处会出现什么单词：
			- ![image.png](../assets/image_1700971005106_0.png)
		- 从模型视角来看，就是输入上下文，模型输出各个单词的出现概率：
			- ![image.png](../assets/image_1700971092564_0.png)
		- 在神经网络中处理单词的方法：
			- 先将单词转化为固定长度的向量（可以采用 one-hot向量表示，即只有一个元素是1，其他元素都是0）:
				- ![image.png](../assets/image_1700971455824_0.png)
			- 神经网络输入层的神经元对应各个单词:
				- ![image.png](../assets/image_1700971519967_0.png)
				- ![image.png](../assets/image_1700971551097_0.png)
	- ## word2vec
		- 略
- # Word2vec 高速化
	- 略
- # RNN
	- 单纯的前馈神经网络无法充分学习时序数据的性质，因此，RNN（循环神经网络， RNN）应运而生。
	- RNN, Recurrent Neural Network
	- RNN拥有环路：
		- ![image.png](../assets/image_1700972267055_0.png)
	- RNN层循环展开：
		- ![image.png](../assets/image_1700972296295_0.png)
	- RNN通过数据的循环，从过去继承数据并传递到现在和未来。因此，RNN层内部获得了记忆隐藏状态的能力。
- # Gated RNN
	- ## RNN 的问题
		- 梯度消失和梯度爆炸
	- ## 梯度消失和LSTM
		- LSTM: Long Short-Term Memory 长短期记忆，也就是可以长时间维持短期记忆。
- # 基于RNN生成文本
	- ## 使用语言模型生成文本
		- 语言模型输出下一个出现的单词的概率分布：
			- ![image.png](../assets/image_1700979381568_0.png)
	- ## seq2seq模型
		- 文本数据、音频数据、视频数据都是时序数据。
		- seq2seq模型也称为 Encoder-Decoder模型，顾名思义，这个模型有两个模块:Encoder（编码器）和Decoder（解码器）
			- ![image.png](../assets/image_1700979697579_0.png)
		- 编码器和解码器协作，将一个时序数据转换为另一个时序数据。在这些编码器和解码器的内部可以使用RNN
	- # Attention
		- ## Attention的结构
			- 注意力机制（Attention mechanism, Attention）是为了进一步强化seq2seq的方法。基于Attention机制，使得seq2seq像人类一样，将“注意力”集中在必要信息上。
			- seq2seq存在的问题：
				- 无论多长的文本，编码器都会将其转换为固定长度的向量，这样有用的信息也可能从向量中溢出。
		- ## Attention的实现
			- 编码器的实现
			- 解码器的实现
				- 解码器的层结构：
					- ![image.png](../assets/image_1700986967051_0.png)
	- ##
	- ## Attention的应用
		- Transformer
		-
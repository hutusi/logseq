# 神经网络的复习
	- ## 数学和Python
		- 向量和矩阵、张量
		- 矩阵运算
		- 广播
	- ## 神经网络的推理
		- 神经网络的例子
			- ![image.png](../assets/image_1700384196845_0.png){:height 373, :width 629}
			- 每个神经元（图中O型节点）的输入是与它连接的上一层神经元的输出值与权重（与连接的两个神经元有关）相乘，并累加，再加上偏置（与上一层神经元无关）；这个值，再经过激活函数转换，成为该神经元的输出值。如下公式(x代表输入层的数据，w代表权重，b代表偏置)：
				- $$ h_1 = x_1w_{11} + x_2w_{21} + b_1 $$
				- 使用 sigmoid激活函数进行变换：
				- $$ a = sigmoid(h) $$
			- 联想：神经网络的推理所进行的处理相当于神经网络的正向传播；神经网络的学习则是反向传播。
				- 将神经网络的层实现为Python 类：
					- 所有的层都有 forward() 方法和 backward() 方法
					- 所有的层都有 params 和 grads 实例变量
				- 神经网络层结构：
				- ![Screenshot 2023-11-20 at 23.14.12.png](../assets/Screenshot_2023-11-20_at_23.14.12_1700493312652_0.png)
	- ## 神经网络的学习
		- 损失函数
			- 在神经网络的学习中，为了知道学习的如何，需要一个指标，这个指标就是损失(loss)。
			- 计算神经网络的损失要用到损失函数(loss function)，进行多类别分类的神经网络通常使用交叉熵误差(cross entropy error)作为损失函数。
			- ![image.png](../assets/image_1700493539753_0.png)
			- 可以将 Softmax 函数和 交叉熵误差的层实现为 Softmax with Loss 层：
			- ![image.png](../assets/image_1700493619661_0.png)
		- 导数和梯度：
			- 导数相当于函数的斜率
			- 将向量的各个元素的导数罗列在一起，就得到了梯度
		- 链式法则：
			- 理解误差反向传播法的关键是链式法则，链式法则是复合函数的求导法则，即复合函数的导数之积。
		- 计算图
			- 使用计算图，可以直观的把握计算过程。
		- 使用计算图对各种节点的正向传播和反向传播进行计算：
			- 加法节点
				- ![image.png](../assets/image_1700576144297_0.png)
			- 乘法节点
				- ![image.png](../assets/image_1700576185370_0.png)
			- 分支节点
				- ![image.png](../assets/image_1700576261718_0.png)
			- Repeat 节点
				- ![image.png](../assets/image_1700576298045_0.png)
			- Sum节点
				- ![image.png](../assets/image_1700576328524_0.png)
			- MatMul节点（Matrix Multiply）
				- ![image.png](../assets/image_1700576384548_0.png)
				- ![image.png](../assets/image_1700576408292_0.png)
		- 神经网络的学习步骤:
			- 步骤1：mini-batch
			- 步骤2：计算梯度
			- 步骤3：更新参数
			- 步骤4：重复1-3
		- 权重更新方法有很多，常见的有 随机梯度下降算法（SGD, Stochastic Gradient Descent）
	- ## 使用神经网络解决问题
	- ## 计算的高速化
		- 位精度：32位浮点数比64位浮点数计算快，16位浮点数比32位浮点数计算快
		- GPU并行计算
-
- # 自然语言和单词的分布式表示
	- ## 自然语言处理
		- NLP = Natural Language Processing
	- ## 同义词词典
		- 同义词词典 WordNet
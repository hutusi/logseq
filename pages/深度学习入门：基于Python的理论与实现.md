- #人工智能
- 一维数组：向量；二维数组：矩阵；三维及以上数组：张量
- ## 感知机：
	- 1957年由罗森布拉特(Frank Rosenblatt)提出。
	- 定义：接收多个输入信号(x)，每个信号有权重(w)，神经元计算输入信号的总和(x*w)，当总和超过设定阈值(θ)时，神经元被激活，输出1。感知机可以表示为如下数学公式：
	- ```
	  $$
	  y=
	  \begin{cases}
	  0 & \quad \text{$(w_1 \times x_1+w_2 \times x_2 \leq \theta)$}\\
	  1 & \quad \text{$(w_1 \times x_1 + w_2 \times x_2 > \theta)$}
	  \end{cases}
	  $$```
	- 可以将阈值 $\theta$ 移到公式左边换成 -b，这里的b即**偏置**:
	- ```
	  $$
	  y=
	  \begin{cases}
	  0 & \quad \text{$(b+w_1 \times x_1+w_2 \times x_2 \leq 0)$}\\
	  1 & \quad \text{$(b+w_1 \times x_1 + w_2 \times x_2 > 0)$}
	  \end{cases}
	  $$```
	- 感知机的实现：与门、与非门、或门；但感知机无法表示异或门。用线性图可以看出，无法画出分割异或门的直线。对此，解决的方案是使用两层感知机来进行组合，即多层感知机。
- ## 神经网络：
	- 神经网络： 输入层 --> 中间层（隐藏层） --> 输出层
	- 对于上文中的公式，引入一个新函数h(x)来简化：$y=h(b+w_1 \times x_1 + w2 \times x_2)$ ，简化后的公式为，这个新函数就是激活函数：
	- ```
	  $$ h(x)=
	  \begin{cases}
	  0 & \quad \text{$(x \leq 0)$}\\
	  1 & \quad \text{$(x > 0)$}
	  \end{cases}
	  $$```
	- h(x) 会将输入信号的总和转换为输出信号，这种函数被称为 **激活函数**。
		- 上述例子中的激活函数可称为阶跃函数
		- 神经网络中经常使用的一个激活函数是 sigmoid 函数(sigmoid function): $h(x)=\dfrac{1}{1+e^{-x}}$
		- 从阶跃函数和sigmoid函数的图形可以看出，阶跃函数以0为界限，输出根据输入急剧变化；而sigmoid函数的输出是平滑的连续性变化。这两个函数都是非线性函数。
			- ReLU函数也是神经网络常用的激活函数：当输入大于0时，直接输出该值；当输入小于等于0时，则输出0。
			  $$ h(x)=
			  \begin{cases}
			  x & \quad \text{$(x > 0)$}\\
			  0 & \quad \text{$(x \leq 0)$}
			  \end{cases}
			  $$
		- 输出层激活函数的选择：一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。
			- 机器学习的问题大致可以分为分类问题和回归问题。分类问题是判断数据属于哪一类别的问题，比如识别图像是猫还是狗之类；而回归问题是根据一个输入预测一个（连续的）数值的问题，比如根据一个人的图像预测该人的体重。
			- 恒等函数会将输入原样输出，即输出层会将输入层的信号原封不动的输出。 $$f(x) = x$$
			- softmax函数表示：
	- 多维数组运算
		- 使用numpy表示多维数组： np.array
		- 矩阵乘法：左边矩阵的行和右边矩阵的列以对应元素的方式相乘再求和而得到。
			- 在矩阵的乘积运算中，对应维度的元素个数要保持一致
		- 神经网络的内积：使用矩阵来表示神经网络（数值为权重）
	- 三层神经网络的实现
		- 输出层的设计：
			- 激活函数：回归问题用恒等函数，分类问题用softmax函数
	- 案例：手写数字识别
		- MNIST数据集：由0-9数字图像构成，训练图像由6万张，测试图像有1万张，用于学习和推理。MNIST的图像数据是28像素*28像素的灰度图像（1通道），各个像素的取值在0-255之间。
		- 神经网络的推理处理：对MNIST数据集进行神经网络设计，该神经网络输入层有784个神经元（28*28=784），输出层有10个神经元（0-9共10个类别的数字），有两个隐藏层：第1个50个神经元，第2个100个神经元。（这两层的神经元个数（50和100）可以设计成任何值。）
		- 批处理：如果将预测函数一次性打包处理100张图像，那么这种打包式的输入数据就称为 批（batch）
			- 批处理可以大幅缩短每张图像的处理时间
- ## 神经网络的学习
	- 神经网络的特征是从数据中学习，即可以由数据自动决定权重参数的值。
		- ![image.png](../assets/image_1699172877214_0.png)
	- 训练数据和测试数据：
		- 使用训练数据进行学习，使用测试数据评价训练得到的模型的实际能力，训练数据也可以称为监督数据
	- 损失函数：
		- 神经网络学习中所使用的指标称为损失函数（loss function），可以使用任意函数，但一般使用均方误差和交叉熵误差等。
			- 均方误差公式：
				- $$E = \dfrac{1}{2}\sum(y_k-t_k)^2$$
			- 交叉熵误差公式：
				- $$E=-\sum_{k}(t_k\log{y_k})$$
	- 数值微分
		- 导数
			- 导数就是表示某个瞬间的变化量，导数可以定义为：
				- $$\dfrac{df(x)}{dx} = \lim_{h \to 0}\dfrac{f(x+h) - f(x)}{h}$$
				- 其中，等号左边符号表示 f(x) 关于 x 的导数，即 f(x) 相对于 x 的变化程度。$$\lim_{h \to 0}$$表示微小变化的h无限趋近于0
-
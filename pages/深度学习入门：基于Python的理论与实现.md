- #人工智能
- 一维数组：向量；二维数组：矩阵；三维及以上数组：张量
- 感知机：
	- 1957年由罗森布拉特(Frank Rosenblatt)提出。
	- 定义：接收多个输入信号(x)，每个信号有权重(w)，神经元计算输入信号的总和(x*w)，当总和超过设定阈值(θ)时，神经元被激活，输出1。感知机可以表示为如下数学公式：
	- ```
	  $$
	  y=
	  \begin{cases}
	  0 & \quad \text{$(w_1 \times x_1+w_2 \times x_2 \leq \theta)$}\\
	  1 & \quad \text{$(w_1 \times x_1 + w_2 \times x_2 > \theta)$}
	  \end{cases}
	  $$```
	- 可以将阈值 $\theta$ 移到公式左边换成 -b，这里的b即**偏置**:
	- ```
	  $$
	  y=
	  \begin{cases}
	  0 & \quad \text{$(b+w_1 \times x_1+w_2 \times x_2 \leq 0)$}\\
	  1 & \quad \text{$(b+w_1 \times x_1 + w_2 \times x_2 > 0)$}
	  \end{cases}
	  $$```
	- 感知机的实现：与门、与非门、或门；但感知机无法表示异或门。用线性图可以看出，无法画出分割异或门的直线。对此，解决的方案是使用两层感知机来进行组合，即多层感知机。
- 神经网络：
	- 神经网络： 输入层 --> 中间层（隐藏层） --> 输出层
	- 对于上文中的公式，引入一个新函数h(x)来简化：$y=h(b+w_1 \times x_1 + w2 \times x_2)$ ，简化后的公式为，这个新函数就是激活函数：
	- ```
	  $$ h(x)=
	  \begin{cases}
	  0 & \quad \text{$(x \leq 0)$}\\
	  1 & \quad \text{$(x > 0)$}
	  \end{cases}
	  $$```
	- h(x) 会将输入信号的总和转换为输出信号，这种函数被称为 **激活函数**。
		- 上述例子中的激活函数可称为阶跃函数
		- 神经网络中经常使用的一个激活函数是 sigmoid 函数(sigmoid function): $h(x)=\dfrac{1}{1+e^{-x}}$
		- 从阶跃函数和sigmoid函数的图形可以看出，阶跃函数以0为界限，输出根据输入急剧变化；而sigmoid函数的输出是平滑的连续性变化。这两个函数都是非线性函数。
			- ReLU函数也是神经网络常用的激活函数：当输入大于0时，直接输出该值；当输入小于等于0时，则输出0。
			  $$ h(x)=
			  \begin{cases}
			  x & \quad \text{$(x > 0)$}\\
			  0 & \quad \text{$(x \leq 0)$}
			  \end{cases}
			  $$
		- 输出层激活函数的选择：一般情况下，回归问题可以使用恒等函数，二元分类问题可以使用sigmoid函数，多元分类问题可以使用softmax函数。
			- 机器学习的问题大致可以分为分类问题和回归问题。分类问题是判断数据属于哪一类别的问题，比如识别图像是猫还是狗之类；而回归问题是根据一个输入预测一个（连续的）数值的问题，比如根据一个人的图像预测该人的体重。
			- 恒等函数会将输入原样输出，即输出层会将输入层的信号原封不动的输出。 $$f(x) = x$$
			- softmax函数表示：
	- 多维数组运算
		- 使用numpy表示多维数组： np.array
		- 矩阵乘法：左边矩阵的行和右边矩阵的列以对应元素的方式相乘再求和而得到。
			- 在矩阵的乘积运算中，对应维度的元素个数要保持一致
		- 神经网络的内积：使用矩阵来表示神经网络（数值为权重）
	-